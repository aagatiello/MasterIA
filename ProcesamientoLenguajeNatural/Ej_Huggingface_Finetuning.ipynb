{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3JYZieDTAHWV"
   },
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fDYHJoDtCT2j"
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizerFast\n",
    "from datasets import load_dataset, load_metric\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-multilingual-cased')\n",
    "train_dataset = load_dataset(\"amazon_reviews_multi\", \"es\", split=\"train\")\n",
    "test_dataset = load_dataset(\"amazon_reviews_multi\", \"es\", split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tH4lo8992bgy"
   },
   "outputs": [],
   "source": [
    "def encode(examples):\n",
    "  return tokenizer(examples['review_body'], truncation=True, padding='max_length')\n",
    "\n",
    "def encode_labels(example):\n",
    "  example['labels'] = example['stars'] - 1\n",
    "  return example\n",
    "\n",
    "train_dataset = train_dataset.map(encode, batched=True)\n",
    "test_dataset = test_dataset.map(encode, batched=True)\n",
    "\n",
    "train_dataset = train_dataset.map(encode_labels)\n",
    "test_dataset = test_dataset.map(encode_labels)\n",
    "\n",
    "train_dataset.set_format(type='torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'labels'])\n",
    "test_dataset.set_format(type='torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-Fv8QLkbAMeq"
   },
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-multilingual-cased\", num_labels=5)\n",
    "\n",
    "for param in model.base_model.parameters():\n",
    "  param.requires_grad = False\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./finetuned/',           # output directory\n",
    "    num_train_epochs=1,                  # total # of training epochs\n",
    "    per_device_train_batch_size=16,      # batch size per device during training\n",
    "    per_device_eval_batch_size=64,       # batch size for evaluation\n",
    "    warmup_steps=500,                    # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,                   # strength of weight decay\n",
    "    logging_dir='./logs/',               # directory for storing logs\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,                         # the instantiated ðŸ¤— Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train_dataset,         # training dataset\n",
    "    eval_dataset=test_dataset            # evaluation dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gyAbyTzGy9hF"
   },
   "outputs": [],
   "source": [
    "trainer.train()\n",
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TVMOLE8ay_55"
   },
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "PLN_SesiÃ³n_13_Huggingface_Finetuning.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
