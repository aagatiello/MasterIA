{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "4. Embeddings.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8gnH3-3rbFmo"
      },
      "source": [
        "## **EJEMPLO DE EMBEDDING**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LOXEKguBY3DX"
      },
      "source": [
        "from numpy import array\n",
        "from keras.preprocessing.text import one_hot\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers.embeddings import Embedding\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xl6MuO7JY5TA"
      },
      "source": [
        "# define documents\n",
        "docs = ['Well done!',\n",
        "\t\t'Good work',\n",
        "\t\t'Great effort',\n",
        "\t\t'nice work',\n",
        "\t\t'Excellent!',\n",
        "\t\t'Weak',\n",
        "\t\t'Poor effort!',\n",
        "\t\t'not good',\n",
        "\t\t'poor work',\n",
        "\t\t'Could have done better.']\n",
        "# define class labels\n",
        "labels = array([1,1,1,1,1,0,0,0,0,0])\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vMSpdE01Y7GH",
        "outputId": "4236c465-549e-4219-d409-c42c1fa46f8d"
      },
      "source": [
        "# integer encode the documents\n",
        "vocab_size = 50\n",
        "encoded_docs = [one_hot(d, vocab_size) for d in docs]\n",
        "print(encoded_docs)\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[12, 17], [29, 47], [25, 30], [5, 47], [37], [45], [12, 30], [44, 29], [12, 47], [42, 21, 17, 30]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w4qHKTJZY8UT",
        "outputId": "5e4ff68e-f8af-4ea9-a880-ff8c320c3605"
      },
      "source": [
        "# pad documents to a max length of 4 words\n",
        "max_length = 4\n",
        "padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
        "print(padded_docs)\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[12 17  0  0]\n",
            " [29 47  0  0]\n",
            " [25 30  0  0]\n",
            " [ 5 47  0  0]\n",
            " [37  0  0  0]\n",
            " [45  0  0  0]\n",
            " [12 30  0  0]\n",
            " [44 29  0  0]\n",
            " [12 47  0  0]\n",
            " [42 21 17 30]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wcxJBNzfY9lL"
      },
      "source": [
        "# define the model\n",
        "# the output from the Embedding layer will be 4 vectors of 8 dimensions each, one for each word. We flatten this to a one 32-element vector to pass on to the Dense output layer.\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 8, input_length=max_length))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qWvKgHc2ZhKX",
        "outputId": "3846574d-a27c-4dfb-abbc-8d7fa19abbe0"
      },
      "source": [
        "# compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "# summarize the model\n",
        "print(model.summary())\n"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_2 (Embedding)     (None, 4, 8)              400       \n",
            "                                                                 \n",
            " flatten_2 (Flatten)         (None, 32)                0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 32)                1056      \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 1)                 33        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,489\n",
            "Trainable params: 1,489\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "usd6n9JBZkn1",
        "outputId": "5cbe76d2-f398-4df2-9196-4fc6bc95a0a5"
      },
      "source": [
        "# fit the model\n",
        "model.fit(padded_docs, labels, epochs=50, verbose=0)\n",
        "# evaluate the model\n",
        "loss, accuracy = model.evaluate(padded_docs, labels, verbose=0)\n",
        "print('Accuracy: %f' % (accuracy*100))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 100.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T3mgpyB_bQv8"
      },
      "source": [
        ""
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ePeam3-bSv3"
      },
      "source": [
        "## **EJEMPLO DE USO DE EMBEDDING PRE-ENTRENADO**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "llqoZ1htbn9i"
      },
      "source": [
        "from numpy import array\n",
        "from numpy import asarray\n",
        "from numpy import zeros\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Embedding"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rTDUwRcFbXBP"
      },
      "source": [
        "# define documents\n",
        "docs = ['Well done!',\n",
        "\t\t'Good work',\n",
        "\t\t'Great effort',\n",
        "\t\t'nice work',\n",
        "\t\t'Excellent!',\n",
        "\t\t'Weak',\n",
        "\t\t'Poor effort!',\n",
        "\t\t'not good',\n",
        "\t\t'poor work',\n",
        "\t\t'Could have done better.']\n",
        "# define class labels\n",
        "labels = array([1,1,1,1,1,0,0,0,0,0])\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NoZfOkbwbd45",
        "outputId": "2aa5a356-fcf8-4c83-b29d-f13abd801445"
      },
      "source": [
        "# prepare tokenizer\n",
        "t = Tokenizer()\n",
        "t.fit_on_texts(docs)\n",
        "vocab_size = len(t.word_index) + 1\n",
        "print(vocab_size)\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "15\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qjASQV3Sbhvc",
        "outputId": "624a7360-9113-455a-8c22-42296f790528"
      },
      "source": [
        "# integer encode the documents\n",
        "encoded_docs = t.texts_to_sequences(docs)\n",
        "print(encoded_docs)\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[6, 2], [3, 1], [7, 4], [8, 1], [9], [10], [5, 4], [11, 3], [5, 1], [12, 13, 2, 14]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vH7oMtwCdFUM",
        "outputId": "84064a20-dd32-4225-e713-09c493ef273c"
      },
      "source": [
        "# pad documents to a max length of 4 words\n",
        "max_length = 4\n",
        "padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
        "print(padded_docs)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 6  2  0  0]\n",
            " [ 3  1  0  0]\n",
            " [ 7  4  0  0]\n",
            " [ 8  1  0  0]\n",
            " [ 9  0  0  0]\n",
            " [10  0  0  0]\n",
            " [ 5  4  0  0]\n",
            " [11  3  0  0]\n",
            " [ 5  1  0  0]\n",
            " [12 13  2 14]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L7tQg3TgdllZ"
      },
      "source": [
        ""
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6H-WNBd1iTM8",
        "outputId": "9c145af2-d579-4e25-f788-928184781c70"
      },
      "source": [
        "# Hay que descargar el embedding\n",
        "# https://nlp.stanford.edu/projects/glove/\n",
        "# Descargamos el de 100 dimensiones\n",
        "\n",
        "# También podemos descargarlo con el siguiente código\n",
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip -q glove.6B.zip"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-11-16 12:01:07--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2021-11-16 12:01:07--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2021-11-16 12:01:07--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  5.03MB/s    in 2m 40s  \n",
            "\n",
            "2021-11-16 12:03:47 (5.14 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j5wTlhsVdP2k",
        "outputId": "134ac07b-0fd5-475c-8bae-818dea093ba2"
      },
      "source": [
        "# load the whole embedding into memory\n",
        "embeddings_index = dict()\n",
        "#f = open('../glove_data/glove.6B/glove.6B.100d.txt')\n",
        "f = open('glove.6B.100d.txt')\n",
        "for line in f:\n",
        "\tvalues = line.split()\n",
        "\tword = values[0]\n",
        "\tcoefs = asarray(values[1:], dtype='float32')\n",
        "\tembeddings_index[word] = coefs\n",
        "f.close()\n",
        "print('Loaded %s word vectors.' % len(embeddings_index))\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 400000 word vectors.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qyES4_qAdP78"
      },
      "source": [
        "# create a weight matrix for words in training docs\n",
        "embedding_matrix = zeros((vocab_size, 100))\n",
        "for word, i in t.word_index.items():\n",
        "\tembedding_vector = embeddings_index.get(word)\n",
        "\tif embedding_vector is not None:\n",
        "\t\tembedding_matrix[i] = embedding_vector\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rdhvW9n6jvwH",
        "outputId": "ece4e737-984b-4bb9-a1bb-016da1224e4d"
      },
      "source": [
        "len(embedding_matrix)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "amxGGYywdQB-"
      },
      "source": [
        "# define model\n",
        "model = Sequential()\n",
        "e = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=4, trainable=False)\n",
        "model.add(e)\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1, activation='sigmoid'))\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2I1CbdTpeMsL",
        "outputId": "12eb1e57-70a7-493b-c64d-deb7b2a80a03"
      },
      "source": [
        "# compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "# summarize the model\n",
        "print(model.summary())\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (None, 4, 100)            1500      \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 400)               0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 1)                 401       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,901\n",
            "Trainable params: 401\n",
            "Non-trainable params: 1,500\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s0La4JZ3eMv5",
        "outputId": "7d980eed-309e-407e-859d-71456ba2071a"
      },
      "source": [
        "# fit the model\n",
        "model.fit(padded_docs, labels, epochs=50, verbose=0)\n",
        "# evaluate the model\n",
        "loss, accuracy = model.evaluate(padded_docs, labels, verbose=0)\n",
        "print('Accuracy: %f' % (accuracy*100))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 100.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ztpQQUp1uaG"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eYSb0R-q1ucj"
      },
      "source": [
        "Ejemplo tomado de la siguiente URL:\n",
        "https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GGQYsI4W1wzb"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}