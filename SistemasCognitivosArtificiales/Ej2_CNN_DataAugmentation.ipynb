{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jYCEIVCXvsNd"
      },
      "source": [
        "# Conseguimos los datos "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n30ZPiooyscu"
      },
      "source": [
        "Importamos el dataset de piedra, papel y tijeras a la carpeta temporal:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "goFtl5r7uNMA",
        "outputId": "f106c569-ab00-47b0-a05e-bfb1fe695449"
      },
      "source": [
        "!wget --no-check-certificate \\\n",
        "    https://storage.googleapis.com/learning-datasets/rps.zip \\\n",
        "    -O /tmp/rps.zip\n",
        "  \n",
        "\n",
        "\n",
        "!wget --no-check-certificate \\\n",
        "    https://storage.googleapis.com/learning-datasets/rps-test-set.zip \\\n",
        "    -O /tmp/rps-test-set.zip"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-05-10 12:12:18--  https://storage.googleapis.com/learning-datasets/rps.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 108.177.119.128, 108.177.126.128, 142.250.153.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|108.177.119.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 200682221 (191M) [application/zip]\n",
            "Saving to: ‘/tmp/rps.zip’\n",
            "\n",
            "/tmp/rps.zip        100%[===================>] 191.38M  45.5MB/s    in 4.5s    \n",
            "\n",
            "2023-05-10 12:12:23 (42.8 MB/s) - ‘/tmp/rps.zip’ saved [200682221/200682221]\n",
            "\n",
            "--2023-05-10 12:12:23--  https://storage.googleapis.com/learning-datasets/rps-test-set.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 108.177.119.128, 108.177.126.128, 142.250.153.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|108.177.119.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 29516758 (28M) [application/zip]\n",
            "Saving to: ‘/tmp/rps-test-set.zip’\n",
            "\n",
            "/tmp/rps-test-set.z 100%[===================>]  28.15M  20.8MB/s    in 1.4s    \n",
            "\n",
            "2023-05-10 12:12:24 (20.8 MB/s) - ‘/tmp/rps-test-set.zip’ saved [29516758/29516758]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0c3cXeQC_43J"
      },
      "source": [
        "Extraemos los datos y los descomprimimos:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OoS4wle03D5a"
      },
      "source": [
        "\n",
        "import os\n",
        "import zipfile\n",
        "\n",
        "local_zip = '/tmp/rps.zip'\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "zip_ref.extractall('/tmp/')\n",
        "zip_ref.close()\n",
        "\n",
        "local_zip = '/tmp/rps-test-set.zip'\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "zip_ref.extractall('/tmp/')\n",
        "zip_ref.close()"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tyka4XO-_70s"
      },
      "source": [
        "Creamos las carpetas para trabajar de forma organizada y las llenamos con los elementos correspondientes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y__lwwXQ3Ft_",
        "outputId": "6099dd68-6327-40a8-d415-18cc4465e16e"
      },
      "source": [
        "rock_dir = os.path.join('/tmp/rps/rock')\n",
        "paper_dir = os.path.join('/tmp/rps/paper')\n",
        "scissors_dir = os.path.join('/tmp/rps/scissors')\n",
        "\n",
        "print('total training rock images:', len(os.listdir(rock_dir)))\n",
        "print('total training paper images:', len(os.listdir(paper_dir)))\n",
        "print('total training scissors images:', len(os.listdir(scissors_dir)))\n"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total training rock images: 840\n",
            "total training paper images: 840\n",
            "total training scissors images: 840\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ePA88x6vwJQO",
        "outputId": "0935ba81-3f96-40d1-dc28-d885a5fa2763"
      },
      "source": [
        "rock_files = os.listdir(rock_dir)\n",
        "print(rock_files[:10])\n",
        "\n",
        "paper_files = os.listdir(paper_dir)\n",
        "print(paper_files[:10])\n",
        "\n",
        "scissors_files = os.listdir(scissors_dir)\n",
        "print(scissors_files[:10])"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['rock06ck02-046.png', 'rock01-020.png', 'rock01-037.png', 'rock01-012.png', 'rock06ck02-053.png', 'rock03-073.png', 'rock06ck02-018.png', 'rock05ck01-034.png', 'rock07-k03-019.png', 'rock05ck01-102.png']\n",
            "['paper06-013.png', 'paper03-066.png', 'paper07-063.png', 'paper03-031.png', 'paper07-115.png', 'paper03-058.png', 'paper01-091.png', 'paper01-043.png', 'paper01-097.png', 'paper06-085.png']\n",
            "['scissors02-113.png', 'scissors04-069.png', 'scissors02-050.png', 'testscissors02-101.png', 'scissors03-002.png', 'scissors03-036.png', 'scissors03-091.png', 'scissors04-077.png', 'testscissors02-007.png', 'scissors01-089.png']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75othCi4ypnl"
      },
      "source": [
        "# Data Augmentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKz7OOAgy-Qt"
      },
      "source": [
        "Definimos las transformaciones a aplicar"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Gm0VhYRzMTd"
      },
      "source": [
        "Enlace con explicación de cada una de las transformaciones \n",
        "https://www.analyticsvidhya.com/blog/2020/08/image-augmentation-on-the-fly-using-keras-imagedatagenerator/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Keras-Preprocessing"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DYet7FD4f0jh",
        "outputId": "2aada171-da5c-458b-e39b-9d622f3adeb7"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: Keras-Preprocessing in /usr/local/lib/python3.10/dist-packages (1.1.2)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.10/dist-packages (from Keras-Preprocessing) (1.22.4)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from Keras-Preprocessing) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "56cmdQZ5wJ0s"
      },
      "source": [
        "import tensorflow as tf\n",
        "import keras_preprocessing\n",
        "from keras_preprocessing import image\n",
        "from keras_preprocessing.image import ImageDataGenerator\n",
        "\n",
        "TRAINING_DIR = \"/tmp/rps/\"\n",
        "training_datagen = ImageDataGenerator(\n",
        "      rescale = 1./255,\n",
        "\t    rotation_range=40,\n",
        "      width_shift_range=0.2,\n",
        "      height_shift_range=0.2,\n",
        "      shear_range=0.2,\n",
        "      zoom_range=0.2,\n",
        "      horizontal_flip=True,\n",
        "      fill_mode='nearest')\n",
        "\n",
        "VALIDATION_DIR = \"/tmp/rps-test-set/\"\n",
        "validation_datagen = ImageDataGenerator(rescale = 1./255)\n",
        "\n"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3mHuKRWqzVXM"
      },
      "source": [
        "Iterador para generar datos sobre directorios"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ugxsoRshzb4C"
      },
      "source": [
        "Enlace donde se explican los parámetros: https://vijayabhaskar96.medium.com/tutorial-image-classification-with-keras-flow-from-directory-and-generators-95f75ebe5720"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oQy-J-M3zAaS",
        "outputId": "4724bf1c-a55c-4ed0-f2d1-ae146ca7ce99"
      },
      "source": [
        "train_generator = training_datagen.flow_from_directory(\n",
        "\tTRAINING_DIR,\n",
        "\ttarget_size=(150,150),\n",
        "\tclass_mode='categorical',\n",
        "  batch_size=126\n",
        ")\n",
        "\n",
        "validation_generator = validation_datagen.flow_from_directory(\n",
        "\tVALIDATION_DIR,\n",
        "\ttarget_size=(150,150),\n",
        "\tclass_mode='categorical',\n",
        "  batch_size=126\n",
        ")"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2520 images belonging to 3 classes.\n",
            "Found 372 images belonging to 3 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "huUVrOdm0iZ9",
        "outputId": "5911e308-e42c-47d2-e298-dfcd1f2867da"
      },
      "source": [
        "rock_dir = os.path.join('/tmp/rps/rock')\n",
        "paper_dir = os.path.join('/tmp/rps/paper')\n",
        "scissors_dir = os.path.join('/tmp/rps/scissors')\n",
        "\n",
        "print('total training rock images:', len(os.listdir(rock_dir)))\n",
        "print('total training paper images:', len(os.listdir(paper_dir)))\n",
        "print('total training scissors images:', len(os.listdir(scissors_dir)))\n"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total training rock images: 840\n",
            "total training paper images: 840\n",
            "total training scissors images: 840\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYIvOvew0s4k"
      },
      "source": [
        "# Creación del Modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o2LB20DU00JM"
      },
      "source": [
        "Diseño del modelo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oR0GkSSV0lrB",
        "outputId": "664365a4-34ee-487e-b823-f228768f6782"
      },
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "    # Note the input shape is the desired size of the image 150x150 with 3 bytes color\n",
        "    # This is the first convolution\n",
        "    tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(150, 150, 3)),\n",
        "    tf.keras.layers.MaxPooling2D(2, 2),\n",
        "    # The second convolution\n",
        "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    # The third convolution\n",
        "    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    # The fourth convolution\n",
        "    #tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
        "    #tf.keras.layers.MaxPooling2D(2,2),\n",
        "    # Flatten the results to feed into a DNN\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    # 512 neuron hidden layer\n",
        "    tf.keras.layers.Dense(512, activation='relu'),\n",
        "    tf.keras.layers.Dense(3, activation='softmax')\n",
        "])\n",
        "\n",
        "\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_11 (Conv2D)          (None, 148, 148, 32)      896       \n",
            "                                                                 \n",
            " max_pooling2d_11 (MaxPoolin  (None, 74, 74, 32)       0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_12 (Conv2D)          (None, 72, 72, 64)        18496     \n",
            "                                                                 \n",
            " max_pooling2d_12 (MaxPoolin  (None, 36, 36, 64)       0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_13 (Conv2D)          (None, 34, 34, 128)       73856     \n",
            "                                                                 \n",
            " max_pooling2d_13 (MaxPoolin  (None, 17, 17, 128)      0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " flatten_3 (Flatten)         (None, 36992)             0         \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 36992)             0         \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 512)               18940416  \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 3)                 1539      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 19,035,203\n",
            "Trainable params: 19,035,203\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ma-vQ6yC039x"
      },
      "source": [
        "Compilación del Modelo, Entrenamiento del Modelo y Guardado de Pesos de los parámetros"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZXZ40N5S02hJ"
      },
      "source": [
        "model.compile(loss = 'categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "    train_generator,\n",
        "    epochs = 20,\n",
        "    validation_data = validation_generator,\n",
        "    verbose = 2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BhABnIUli0N-",
        "outputId": "51b44e8d-1d39-4f1f-c2dc-26aaeb7db7fd"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "20/20 - 32s - loss: 2.4031 - accuracy: 0.3437 - val_loss: 1.0814 - val_accuracy: 0.3333 - 32s/epoch - 2s/step\n",
            "Epoch 2/20\n",
            "20/20 - 21s - loss: 1.0800 - accuracy: 0.4052 - val_loss: 1.0718 - val_accuracy: 0.3575 - 21s/epoch - 1s/step\n",
            "Epoch 3/20\n",
            "20/20 - 21s - loss: 1.1394 - accuracy: 0.4385 - val_loss: 0.9669 - val_accuracy: 0.6532 - 21s/epoch - 1s/step\n",
            "Epoch 4/20\n",
            "20/20 - 22s - loss: 0.9713 - accuracy: 0.5302 - val_loss: 0.9703 - val_accuracy: 0.5161 - 22s/epoch - 1s/step\n",
            "Epoch 5/20\n",
            "20/20 - 23s - loss: 0.9383 - accuracy: 0.5853 - val_loss: 0.5504 - val_accuracy: 0.7446 - 23s/epoch - 1s/step\n",
            "Epoch 6/20\n",
            "20/20 - 21s - loss: 0.7709 - accuracy: 0.6516 - val_loss: 0.5027 - val_accuracy: 0.8118 - 21s/epoch - 1s/step\n",
            "Epoch 7/20\n",
            "20/20 - 22s - loss: 0.6945 - accuracy: 0.6817 - val_loss: 0.7283 - val_accuracy: 0.6425 - 22s/epoch - 1s/step\n",
            "Epoch 8/20\n",
            "20/20 - 21s - loss: 0.6290 - accuracy: 0.7294 - val_loss: 0.9296 - val_accuracy: 0.5430 - 21s/epoch - 1s/step\n",
            "Epoch 9/20\n",
            "20/20 - 22s - loss: 0.5686 - accuracy: 0.7476 - val_loss: 0.3305 - val_accuracy: 0.8172 - 22s/epoch - 1s/step\n",
            "Epoch 10/20\n",
            "20/20 - 22s - loss: 0.5996 - accuracy: 0.7560 - val_loss: 0.4403 - val_accuracy: 0.8091 - 22s/epoch - 1s/step\n",
            "Epoch 11/20\n",
            "20/20 - 21s - loss: 0.4840 - accuracy: 0.7956 - val_loss: 0.1670 - val_accuracy: 0.9839 - 21s/epoch - 1s/step\n",
            "Epoch 12/20\n",
            "20/20 - 21s - loss: 0.3691 - accuracy: 0.8563 - val_loss: 0.2465 - val_accuracy: 0.8978 - 21s/epoch - 1s/step\n",
            "Epoch 13/20\n",
            "20/20 - 22s - loss: 0.4675 - accuracy: 0.8179 - val_loss: 0.1609 - val_accuracy: 0.9785 - 22s/epoch - 1s/step\n",
            "Epoch 14/20\n",
            "20/20 - 22s - loss: 0.3368 - accuracy: 0.8714 - val_loss: 0.1290 - val_accuracy: 0.9704 - 22s/epoch - 1s/step\n",
            "Epoch 15/20\n",
            "20/20 - 26s - loss: 0.2263 - accuracy: 0.9206 - val_loss: 0.1261 - val_accuracy: 0.9543 - 26s/epoch - 1s/step\n",
            "Epoch 16/20\n",
            "20/20 - 21s - loss: 0.3265 - accuracy: 0.8925 - val_loss: 0.0940 - val_accuracy: 0.9651 - 21s/epoch - 1s/step\n",
            "Epoch 17/20\n",
            "20/20 - 22s - loss: 0.2285 - accuracy: 0.9159 - val_loss: 0.0725 - val_accuracy: 0.9839 - 22s/epoch - 1s/step\n",
            "Epoch 18/20\n",
            "20/20 - 22s - loss: 0.2064 - accuracy: 0.9226 - val_loss: 0.3008 - val_accuracy: 0.8602 - 22s/epoch - 1s/step\n",
            "Epoch 19/20\n",
            "20/20 - 21s - loss: 0.2654 - accuracy: 0.9091 - val_loss: 0.1325 - val_accuracy: 0.9462 - 21s/epoch - 1s/step\n",
            "Epoch 20/20\n",
            "20/20 - 22s - loss: 0.1726 - accuracy: 0.9397 - val_loss: 0.3003 - val_accuracy: 0.8978 - 22s/epoch - 1s/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JKP482xmk5Oc"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_xFcLxxAOnb"
      },
      "source": [
        "Hacemos un grafico con la precisión de entrenamiento y validación:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6nHmxIXK3M-W"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(len(acc))\n",
        "\n",
        "plt.plot(epochs, acc, 'r', label='Training accuracy')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation accuracy')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend(loc=0)\n",
        "plt.figure()\n",
        "\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GueLa27M1YYW"
      },
      "source": [
        "# Uso del modelo para predecir imágenes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7d6HJoPP3O90"
      },
      "source": [
        "import numpy as np\n",
        "from google.colab import files\n",
        "from keras.preprocessing import image\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        " \n",
        "  # predicting images\n",
        "  path = fn\n",
        "  img = image.load_img(path, target_size=(150, 150))\n",
        "  x = image.img_to_array(img)\n",
        "  x = np.expand_dims(x, axis=0)\n",
        "\n",
        "  images = np.vstack([x])\n",
        "  classes = model.predict(images, batch_size=10)\n",
        "  print(fn)\n",
        "  print(classes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lmq9rccGh23G"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}